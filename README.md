# Hadoop, Hive, and Spark Project

## Project Overview
This project demonstrates the usage of Hadoop, Hive, and Spark components. It includes a setup of Hadoop components like NameNode, DataNode, ResourceManager, NodeManager, and History Server. Additionally, it incorporates Hive components like Hive Server, Hive Metastore, Hive Metastore PostgreSQL, and Spark components like Spark Master and Spark Worker.

## Components Used

### Hadoop Components
- **NameNode**
- **DataNode**
- **ResourceManager**
- **NodeManager**
- **History Server**

### Hive Components
- **Hive Server**
- **Hive Metastore**
- **Hive Metastore PostgreSQL**

### Spark Components
- **Spark Master**
- **Spark Worker**

## Usage
### Hive Usage
For detailed information on how to use Hive components in this project, please refer to the [`for-hive.md`](for-hive.md) file.

### Spark Usage
For detailed information on how to use Spark components in this project, please refer to the [`for-spark.md`](for-spark.md) file.

## Getting Started
Follow the steps below to get started with the project:

1. **Clone the Repository**
   ```bash
   git clone https://github.com/waltzofflowers/hadoop_hive_spark.git
   cd hadoop_hive_spark
    ```

2. **Installation and Setup**

    Docker Installation Guide: [`Docker Installation Guide`](https://docs.docker.com/get-started/get-docker/)

3. **Build and Start the Containers**

    ```bash
   docker-compose up -d --build
    ```
4. **Verify the Setup**

    Ensure all the services (Hadoop, Hive, Spark) are running properly by checking the logs or accessing the web interfaces if available.

## Contributing

If you would like to contribute to this project, please fork the repository and create a pull request with your changes.

## Acknowledgments

Special thanks to the contributors and maintainers of the Hadoop, Hive, and Spark communities.

## Contact

For any questions or contributions, please contact cihat.burak.uluturk@gmail.com .
